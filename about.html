---
layout: page
title: About
permalink: /about/
---
<!-- Slider Start -->
<section id="global-header">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <div class="block">
          <h1>About MultiTrans project</h1>
          <h4>Gradual Multi Transfer Learning for Safe Autonomous Driving</h4>
        </div>
      </div>
    </div>
  </div>
</section>




<section id="context">
	<div class="container">
		<div class="row">
		      <div class="section-title">
			<h2>Context</h2>
			</div>
			<p>
			The large-scale implementation stage of autonomous (a.k.a. self-driving, driver-less or robotic) vehicles (AVs) seems to be irremediably and regularly postponed as designing safe and reliable systems able to drive in open environments remains extremely challenging. Recent studies suggest technologies would not be mature before several decades [LitmanÂ 2017] and that the legal and ethical concerns may incur additional delays [Martinez-Diaz et al. 2018]. Research efforts have been carried out in enabling AVs technologies to be operational by taking advantage of the recent advances in Artificial Intelligence (AI). In AI-based approaches, ensuring a system autonomy usually requires to tackle three critical steps: perception, decision and control that can each be designed as specific but inter-dependent algorithms [Bojarski et al. 2016; Lee et al. 2017; Zeng et al. 2019]. This allows researchers to be focused on a part of the big picture while contributing to the main objective: enabling robot-cars to outperform human drivers. Before reaching this long-term target, research is needed and we specifically imagined this project to contribute to the perception stage of AVs systems. Our assumption is that the latter two steps (decision and control) are there particularly tied to the output of the perception stage which needs to provide a very accurate representation of the driving environment(s) while allowing a clear discrimination between similar but different contexts.
		        </p>
		      
		    </div>
		</div>
	</div>
</section>

<section id="research-challenges">
	<div class="container">
		<div class="row">
		      <div class="section-title">
			<h2>Research challenges</h2>
			</div>
			<p>The project takes the perspective of vision-based embedded systems (i.e., relying on cameras or similar sensors) that are among the most promising perception solutions. Their underlying sensing technologies however make them sensitive to an important research challenge: <span class="bolded">(C1) facing adverse conditions</span> (such as bad weather or sun glare).
Designing an AI-based (and especially a learning-based) system gives a strong importance to the amount of data (a.k.a. experience) required for algorithms to converge to a suitable solution that covers the wide range of situations (contexts or domains) the system will face. AVs platforms (or experimental equipped vehicles) being precious and limited resources that often lack the legal framework to allow to operate everywhere, the amount of data gathered and the range of domains they are tested in is inherently restricted. For this reason, development often goes through a simulation stage or a testing step on a simplified system (e.g., smaller vehicles, standalone sensors or robotic models). While this allows to artificially (or virtually) extend the range of contexts the learning-based system is trained in; it also faces two important research challenges that are clearly identified: <span class="bolded">(C2) reality gap</span>, when a simulation/model fails to capture all the particularities of a real system, and <span class="bolded">(C3)</span> extended development time caused by the inherent repeated iterative process of <span class="bolded">adapting an algorithm from a system/domain to a different one.</span></p>
		      </div>
		    </div>
		</div>
	</div>
</section>

<section id="objectives">
	<div class="container">
		<div class="row">
		      <div class="section-title">
			<h2>Objectives</h2>
			</div>
			<p>The main objectives of this project are :
		      	<ul>
		      	    <li><p>Dealing with adverse conditions in vision- and learning-based AVs perception: by modelling or reproducing such environment/conditions in a robotic model/simulation and reusing previously learnt strategies to cope with missing or altered images/situations.</p></li>
		      	     <li><p>Reducing reality gap between simulation/robotic models and AVs: by introducing an intermediate environment and allowing algorithms to be transferred from one (virtual or real) world to another.</p></li>
		      	     <li><p>Enabling domain adaptation to accelerate AVs algorithms development and deployment: by investigating state-of-the-art and novel techniques akin to Transfer Learning and domain/context transfer.</p></li>
		      	    
		      	</ul>
		        </p>
		      
		    </div>
		</div>
	</div>
</section>

<section id="methodology">
	<div class="container">
		<div class="row">
		      <div class="section-title">
			<h2>Methodology</h2>
			</div>
			<p>In MultiTrans, we propose to tackle autonomous driving algorithms development and deployment jointly. </p><p class="bolded">The idea is to enable data, experience and knowledge to be transferable across the different systems (simulation, robotic models, and real-word cars), thus potentially accelerating the rate an embedded intelligent system can gradually learn to operate at each deployment stage.</p><p>The research hypothesis acting as a starting point of MultiTrans corresponds to the current state of deployment of autonomous driving technologies: AVs can be programmed (or are able to learn) to react and operate in controlled (or restricted) environments autonomously. Research is needed to help these systems during the perception stage, enabling them to be operational and safer in a wider range of situations.</p>
		      
		    </div>
		</div>
	</div>
</section>

<section id="impacts">
	<div class="container">
		<div class="row">
		      <div class="section-title">
			<h2>Expected impacts and benefits of the project</h2>
			</div>
			<p>The project is expected to contribute to substantial advances with respect to state of the art, by resulting in:</p>
			<ol>
    <li><p> A novel theoretical framework and new algorithms on transfer and frugal learning in virtual and real environments</p></li>
    <li><p>Advances in multi-domain and multi-source computer vision for semantic segmentation and scene recognition applied to safe autonomous driving</p></li>
    <li><p>The development of a robotic autonomous vehicle model demonstrator combined with a virtual world model</p></li>
    
    			</ol>
		      
		    </div>
		</div>
	</div>
</section>
